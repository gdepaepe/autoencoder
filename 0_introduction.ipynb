{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42837027-ef0c-4c32-8f77-c30f88f22e59",
   "metadata": {},
   "source": [
    "# Machine learning for finding similar weather patterns in big climate datasets\n",
    "For climate science it is sometimes interesting to look for analogue synoptic situations. This could be usefull for finding similar extreme events or when you would like to study changes in the occurrence of certain weather patterns (e.g. dunkelflautes). The problem is that these datasets can be very lange and that searching for similar patterns can be computational intensive and time consuming. In this project a convolutional autoencoder is used for compressing the dataset. An autoencoder is a deep learning neural network that consits of 2 parts: an encoding and a decoding part. \n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/3/37/Autoencoder_schema.png\" width=200 height=200 />\n",
    "\n",
    "Autoencoders are used for dimension reduction, as can also be done with principal component analysis. The power of autoencoders is lying in its non-linearity. Autoencoder are also for for noise reduction in images. The autoencoder architecture used here is based on an image processing autoencoder. It uses convolute layers which are able to detect spacial relationships within pictures or maps. The autoencoder is trained so that the input is matching the output.\n",
    "\n",
    "After training and testing the neural network, the dataset is compressed with the encoder. Similar weather patterns are searched in compressed space. It is shown that \"nearby weather patterns\" in compressed space are also \"nearby\" in \"real nature\" and thet the correlation between the distance in compressed and uncompressed space is high.\n",
    "\n",
    "This project is using Jupyter Lab as a tool for coding and documentation. Google Vertex AI Workbench was used as cloud based Jupyter Lab environment. A user managed notebook with a 8 vCPU (the maximum with educational credits) and 52 GB RAM machinetype has been chosen. \n",
    "\n",
    "The notebook structure is as follows:\n",
    "- 0. Introduction: this notebook\n",
    "- 1. Data preparation: loading, cuting zone of interest, scaling and saving the data in a numpy file\n",
    "- 2. Train autoencoder: creating train, validation and test datasets, setting the neural network architecture, training and testing the neural network\n",
    "- 3. Compress the dataset: splitting the autoencoder model in a compress and decode model, compressing the ERA-20C data\n",
    "- 4. Searching for similar weather patterns: calculating distances in compressed space and searching for similar patterns\n",
    "- 5. Further improving the model with a custom loss function\n",
    "- 6. How about wheather prediction: building a simple pridiction model\n",
    "- 7. Annex 1 - Toolkit for this project: installing the toolkit, plotting function\n",
    "- 8. Annex 2 - Searching with the EMSLP dataset: using a more \"lightweight\" dataset\n",
    "- 9. References\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m93"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
