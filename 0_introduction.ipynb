{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42837027-ef0c-4c32-8f77-c30f88f22e59",
   "metadata": {},
   "source": [
    "# Machine learning for finding similar weather patterns in big climate datasets\n",
    "For climate science, it is interesting to be able to search for analogue synoptic situations. This is useful for finding similar extreme events, like the 2021 July blocking depression, causing the flood in the Vesdre area. It allows also to study changes in the occurrence of certain weather patterns (e.g. dunkelflautes). The problem is that climate datasets are usually very large and searching for similar patterns can be computational intensive and time consuming. In this project a convolutional autoencoder is used for compressing the dataset. An autoencoder is a deep learning neural network (NN) that consists of 2 parts: an encoding and a decoding part. \n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/3/37/Autoencoder_schema.png\" width=200 height=200 />\n",
    "\n",
    "Autoencoders are used for dimension reduction, as can also be done with principal component analysis. The power of autoencoders is coming from the fact that NN are using non-linear activation functions. Autoencoder are also for noise reduction in images. The autoencoder architecture used here is based on an image processing autoencoder. It uses convolute layers which are able to detect spatial relationships within pictures or maps. \n",
    "\n",
    "The autoencoder is trained so that the input is matching the output. After training and testing the neural network, the dataset is compressed with the encoder part. Similar weather patterns are searched in compressed space. It is shown that \"nearby weather patterns\" in compressed space are also \"nearby in real nature\" and that the correlation between the distance in compressed and uncompressed space is high.\n",
    "\n",
    "As a teaser for possible future work, the project ends with a simple forecast model, based on a Long short-term memory (LSTM) neural network.\n",
    "\n",
    "Jupyter Lab is used as a tool for coding and documentation. Google Vertex AI Workbench was used as cloud based Jupyter Lab environment. A user managed notebook with a 8 vCPU (the maximum with educational credits) and 52 GB RAM machine type has been chosen. \n",
    "\n",
    "The notebook structure is as follows:\n",
    "- 0. Introduction: this notebook\n",
    "- 1. Data preparation: loading, cutting the zone of interest, scaling and saving the data in a numpy file\n",
    "- 2. Train autoencoder: creating train, validation and test datasets, setting the neural network architecture, training and testing the NN\n",
    "- 3. Compress the dataset: splitting the autoencoder model in a compress and decode model, compressing the ERA-20C data\n",
    "- 4. Searching for similar weather patterns: calculating distances in compressed space and searching for similar patterns\n",
    "- 5. Further improving the model with a custom loss function\n",
    "- 6. How about weather prediction: building a simple forecast model\n",
    "- 7. Annex 1 - Toolkit for this project: installing the toolkit, plotting function\n",
    "- 8. Annex 2 - Searching with the EMSLP dataset: using a more \"lightweight\" dataset\n",
    "- 9. References\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m93"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
